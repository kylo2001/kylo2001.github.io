---
layout: post
title:  " Very Deep Convolutional Networks for Large-Scale Image Recognition "
categories: Paper
tags: Classification
author: GGuJi


---

* content
{:toc}

# VGGNet

Paper: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)

References:

- [https://m.blog.naver.com/laonple/220749876381](https://m.blog.naver.com/laonple/220749876381)
- [https://89douner.tistory.com/61](https://89douner.tistory.com/61)
- [https://oi.readthedocs.io/en/latest/computer_vision/cnn/vggnet.html](https://oi.readthedocs.io/en/latest/computer_vision/cnn/vggnet.html)
- [https://herbwood.tistory.com/7](https://herbwood.tistory.com/7)
- [https://codebaragi23.github.io/machine%20learning/1.-VGGNet-paper-review/](https://codebaragi23.github.io/machine%20learning/1.-VGGNet-paper-review/)

---

## 1. Background

![스크린샷 2022-08-31 오후 8.42.52.png](https://user-images.githubusercontent.com/20294786/187699478-fa85eea2-e7d2-4e47-9f50-57694149a5cd.png)

**2014**년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)대회에 **GoogLeNet**
과 함께 등장하여 그해 준우승을 거둔 모델이다.

![스크린샷 2022-08-31 오후 8.43.21.png](https://user-images.githubusercontent.com/20294786/187699207-8708976b-ca92-4c50-9bbb-62bee91864aa.png)

GoogLeNet과 마찬가지로 AlexNet에 비해서 더 깊은 layer를 쌓으면서 뛰어난 성능을 보여주기 시작한 모델이기도 하다. 우승은 GoogLeNet이 했지만, 상대적으로 간편한 모델 구조를 가지고 있어 VGG가 base model로 많이 활용되고 있다.

GoogLeNet 저자 Szegedy는 VGGNet의 단점을 다음과 같이 말했다. “파라미터의 개수가 너무 많다”. 

위를 보면 알 수 있는 것처럼,

AlexNet은 6천만개, GoogLeNet는 5백만개 수준이었던 것에 비해 VGGNet은 가장 단순한 A-구조에서는 파라미터의 개수가 1억 3천 3백만개 그리고 VGGNet-16 모델의 경우 1억 3천 8백만개로 엄청나게 많다.

그 결정적인 이유는 VGGNet의 경우는 AlexNet과 마찬가지로 최종단에 fully-connected layer 3개가 오는데 이 부분에서만 파라미터의 개수가 약 1억2천2백만개가 온다. 참고로 GoogLeNet은 Fully-connected layer가 없다.

- **Convolution layer**
- **Pooling layer**
- **Fully connected layer**

하지만, VGGNet이 CNN 학습의 표준모델로 자리잡을 수 있었던 이유는 위와 같은 CNN의 기본 아키텍처만을 사용하여 만들었기 때문이다.

---

## 2. Feature

![스크린샷 2022-08-31 오후 8.47.07.png](https://user-images.githubusercontent.com/20294786/187699220-0a978f8a-6caa-4f84-a2de-1442e8eb401d.png)

Abstract 상에서 언급되는 VGGNet의 대표적인 **특징**으로는 다음 세가지 라고 볼 수 있다.

- **large scale image recognition**
- **3x3 convolution filter**
- **increasing depth**

---

## 3. Model

![스크린샷 2022-08-31 오후 8.48.07.png](https://user-images.githubusercontent.com/20294786/187699227-ed0d277b-4b17-4074-a119-9d73b1005d48.png)

입력 사이즈는 224x224 크기의 RGB 영상을 사용하였다. training set 전체의 mean RGB 값을 입력 영상의 각 pixel마다 빼서 zero-centered되게 한다.

- 13 Convolution Layers + 3 Fully-connected Layers
- **3x3 convolution filters**
- stride: 1 | padding: 1
- 2x2 max pooling (stride: 2)
- ReLU
- Softmax

위와 같은 구조로 모델을 구성하였다. 

![7 x 7 Filter](https://user-images.githubusercontent.com/20294786/187699229-e0d5b0d3-0832-49e4-b17d-f159b3820127.png)

7 x 7 Filter

![3 x 3 Filter](https://user-images.githubusercontent.com/20294786/187699232-4a7e4ed0-6675-4485-a9aa-60fcd4a10eb4.png)

3 x 3 Filter

기존처럼 7 x 7 필터를 이용하여 Convolution을 했을 경우 특징 맵의 각 픽셀 당 Receptive Field는 7 x 7이다. stride가 1일 때, 3차례의 3 x 3 Conv layer를 반복한 특징 맵의 한 픽셀당 원본 이미지의 7 x 7 Receptive Field의 효과를 볼 수 있다. 

> 여기서 **Receptive Field**란?
> 

영상에서 특정 위치에 있는 픽셀들은 그 주변에 있는 일부 픽셀들 하고만 correlation이 높고 거리가 멀어질수록 그 영향은 감소하게 된다. 이를 이용해서 영상이나 이미지를 해석하여 “인식 알고리즘"을 수행하고자 할 경우 영상 전체 영역에 대해 서로 동일한 중요도를 부여하여 처리하는 대신에 특정 범위를 한정해 처리를 하면 훨씬 효과적일 것이라 짐작 할 수 있다. 이를 영상에만 한정하는 것이 아니라 locality를 갖는 모든 신호들에 유사하게 적용할 수 있다는 아이디어에 기반하여 **CNN이 출현**하게 된 것인데, 특정 영역을 한정하기 위해 filter를 사용하게 되는데 filter에 포함되는 영역을 Receptive Field라고 한다.

이전까지는 비교적 큰 Receptive Field를 갖는 11 x 11, 7 x 7 필터를 포함하였는데,

VGG는 오직 3 x 3 크기의 작은 필터만 사용했음에도 비약적으로 개선시켰다.

그럼 왜 3 x 3 Filter를 사용한 것 일까???

저자는 **3 x 3 필터를 사용한 이유**에 대해서 설명하고 있다.

첫 번째 이유, 비선형 함수인 ReLU함수를 더 많이 사용하여 **비선형성을 증가**시킨다.

두 번재 이유, **파라미터 수의 감소**로 오버피팅을 줄여준다.

Conv layer 구조를 학습할 때는 학습 대상인 가중치(weight)는 필터의 크기이다.

7 x 7 필터 1개에 대한 학습 파라미터 수는 49 이고,

3 x 3 필터 3개에 대한 학습 파라미터 수는 27(9 * 3) 이다.

파라미터 수가 크게 감소하는 것을 알 수 있다.

---

## 4. Training

![스크린샷 2022-08-31 오후 9.03.59.png](https://user-images.githubusercontent.com/20294786/187699239-6410ac42-50cb-4d32-8292-5a98c106efdf.png)

- Cost Function: Multinomial logistic regression(Cross Entropy)
- Mini-batch: batch size 256
- Optimizer: Momentum 0.9
- Regularization: L2 Regularization 5*10^-4 & Dropout 0.5
- Learning rate: 10^-2
    - validation error rate이 높아질수록 1/10이 감소
    

Training을 할 때는 위와 같은 설정을 하고 진행하였다. 

AlexNet보다도 더 깊고 파라미터도 많지만 더 적은 epoch를 기록했다고 언급하면서 그 이유를 implicit regularisation과 pre-initialisation이라고 하고 있다.

![스크린샷 2022-08-31 오후 9.04.31.png](https://user-images.githubusercontent.com/20294786/187699243-7c5f03e9-9095-4379-bf8a-ec0067eb0800.png)

**implicit regularisation**는 “This can be seen as imposing a regularisation on the 7 × 7 conv. filters”라고 논문에서 언급한 것을 보면 7 x 7 filter 대신 3 x 3 filter를 사용함으로써 파라미터 수를 줄일 수 있기 때문에 이를 표현한 것이다. 

~~explicit regularisation는 Dropout 0.5이다.~~

**pre-initialisation**는 VGG A모델을 먼저 학습 시켜 B, C, D, E 모델들을 구성 할때 A 모델에 학습된 layer들을 가져다가 쓰는 방식을 말하는 것이다.

이러한 방법을 통해 최적의 초기값을 설정해주어 학습을 용이하게 만든 것 같다.

![스크린샷 2022-08-31 오후 9.59.51.png](https://user-images.githubusercontent.com/20294786/187699246-7687d3bc-fe05-4511-92ba-d55a13c2595e.png)

VGG 모델을 학습시킬 때 제일 처음하는 것은 training image를 VGG 모델의 input size에 맞게 바꿔주어야 한다는 것이다.

![스크린샷 2022-08-31 오후 10.00.30.png](Rhttps://user-images.githubusercontent.com/20294786/187700317-971de0ed-62d6-47a4-b6a6-2371000456da.png)

저자는 **isotropically-rescale**를 ****해주었다고 언급하는데 이는 넓이, 높이 중에서 더 작은 side를 S라는 고정된 사이즈로 줄여주는데 이때 aspect ratio를 유지하면서 나머지 side도 rescaling해주는 방식을 말한다.

S사이즈로 설정한 후 rescaling된 이미지 상에서 224 x 224 크기로 random crop을 진행한다.

> **single-scale training** vs **multi-scale training**
> 

Training시 S값을 설정하는 방법에는 두 가지가 있다. 첫 번째가 single scale training방식이다. 이 방식은 S를 256 or 384로 고정시켜 사용하는 방식이다. 처음에는 S=256으로 설정하여 학습시킨 VGG 모델의 가중치값들을 기반으로 S=384으로 설정하여 다시 학습시킨다. S=256에서 이미 많은 학습이 진행된 상태이기 때문에 S=384로 설정하고 학습할때는 learning rate를 줄여주고 학습을 시켜준다. 

또 다른 방식은 multi scale training이다. 이번에는 S를 고정시키지 않고 256~512 값들 중에서 random하게 값을 설정해준다. 보통 객체들이 모두 같은 사이즈가 아니라 각각 다를 수 있기 때문에 이렇게 random하게 multi scale로 학습시키게 해주면 학습효과가 더 좋아질 수 있다.

![스크린샷 2022-08-31 오후 10.10.11.png](https://user-images.githubusercontent.com/20294786/187700483-55e1a56e-546e-4284-8783-dbd2939354d3.png)

그리고 보통 이러한 data augmentation 방식을 **scale jittering**이라고도 한다.

이러한 multi scale을 학습시킬 때에는 앞선 single scale training 방식(S=256 or 384)을 진행한 VGG model로 fine tuning 시키게 된다.

---

## 5. Test

- Single scale evaluation
- Multi scale evaluation
- Multi crop evaluation
- Dense evaluation
- Multi crop + Dense evaluation

연구팀은 위와 같은 총 5가지의 실험을 했다.

### Single/Multi Scale Evaluation

![스크린샷 2022-08-31 오후 10.21.05.png](https://user-images.githubusercontent.com/20294786/187699263-77969f04-d4bb-43b5-9a02-97633171aef3.png)

Training image가 rescaling된 것처럼 test(validation) image에 대해서도 rescaling 되는데, Training image를 rescaling할때 기준이 되는 값을 S라고 했다면, test image를 rescaling하는 기준은 Q라고 한다.

이때  S와 Q가 같을 필요는 없고, 각 S에 대해서 다른 Q값을 적용했을때 성능이 좋아졌다고 언급하고 있다.

만약 Q가 단일 값이라면 Single scale evaluation, 여러개의 값이라면 Multi scale evaluation이라고 한다.

### Multi crop evaluation

VGGNet의 multi crop evaluation을 보기 전에 AlexNet과 GoogLeNet의 Multi crop 방식을 알아보자.

- **AlexNet**
    - Data augmentation
        - 256x256으로 Scale하고 코너와 중앙에서 224x224 크기로 Crop하여 5개 이미지 생성
        - 위 결과를 좌우 반전하여 5개 이미지를 더 생성하여 총 10개 이미지 생성
    - Prediction
        - 10개 이미지를 모델에 입력해 나온 10개의 결과를 평균하여 최종 결과 생성
        - Softmax 결과가 숫자로 나오므로 이를 평균하여 최종 Class 결정
- **GoogLeNet**
    - Data augmentation
        - 이미지를 4개의 Scale (256, 288, 320, 352)로 생성
        - 각 Scale 결과에서 3장의 정사각형 이미지 선택
        - 위 결과의 각 이미지에서 4개 코너, 2개 중앙을 추출해 6장의 224x224 이미지 추출
        - 위 결과를 좌우 반전하여 이미지 추출
        - 최종적으로 하나의 이미지에서 4x3x6x2 = 144개의 이미지를 추출
    - Prediction
        - 결과는 AlexNet처럼 Voting 활용

![스크린샷 2022-08-31 오후 10.28.55.png](https://user-images.githubusercontent.com/20294786/187699266-6bf02388-ea4a-4d59-94da-5e782405d71b.png)

VGGNet은 GoogLeNet의 Multi crop 방식과 유사하게 적용하여 하나의 이미지에 대해서 150장의 이미지를 추출하는 multi crop 방식을 적용하였다.

### Dense evaluation

VGGNet에서는 또 dense evaluation을 적용하였는데

![**기존의 Max pooling**](https://user-images.githubusercontent.com/20294786/187700728-6076cf12-bfb5-464e-9010-bab56f1e4188.png)

**기존의 Max pooling**

![**OverFeat이 제안하는 Max Pooling**](https://user-images.githubusercontent.com/20294786/187699277-fb9c5151-c54c-4793-aadc-c59f6b9661bc.png)

**OverFeat이 제안하는 Max Pooling**

Detection 관련해 R-CNN 이후에 나온 **OverFeat**(Integrated Recognition, Localization and Detection Using Convolutional Networks)구조에서 사용한 dense evaluation 개념을 적용한 것이다.

pooling 연산을 거쳐서 나온 feature map이 기존에는 원본 이미지 상에서 지나치게 넓은 receptive field를 표현하다보니 오히려 객체를 제대로 포착하지 못한다고 이해할 수 있는데 OverFeat 저자는 {0, 1, 2}의 feature map pixel offset 조합에 따라 dense 하게 pooling을 진행할 수 있도록 하여 좀 더 해상력을 높이도록 제안하였다.

evaluation중에 앞서 봤던 multi crop과 이 dense evaluation이 서로 complementary해서 이 둘을 적절하게 섞어서 사용했을때 가장 좋은 성능을 보였다고 언급하고 있다.

> **Complementary??**
> 

내가 이해한 바로는 ****Multi crop 방식은 원본 이미지 상에서 random하게 원본 이미지 상의 특정 부분을 crop하기 때문에 OverFeat에서 제안하는 방식에 비해서 **non-spatial**하다고 할 수 있는 것 같다.

전체 이미지를 dense하게 볼 수 있는 dense 방식은 spatial하다고 할 수 있기 때문에 서로 보완을 해줄 수 있다고 이해했다.

![스크린샷 2022-08-31 오후 10.33.42.png](https://user-images.githubusercontent.com/20294786/187699283-6fa74227-2472-4c98-87f7-36da3a666cb4.png)

**“Sermanet et al., 2014”**와 유사하게 test를 하는 모델의 구조를 변경하는데 그 이유를 모르겠다만,,,

첫 FC layer는 7 x 7 conv 그리고 마지막 두 개의 FC layer는 1 x 1 conv로 변경한다. 이렇게 함으로서 uncropped image에 대해서 볼 수 있다고 언급하고 있다.

Conv layer는 입력 이미지 사이즈에 제한이 없어지기 때문에 고정된 크기로 입력을 하지 않아도 되기 때문에 전체 이미지를 그대로 사용함으로서 uncropped image에 대해서 볼 수 있다고 언급하는 것 같다. 

> Conv layer는 입력 이미지의 사이즈가 제한이 없다???
> 

![**ConvNets input size constraints**](https://user-images.githubusercontent.com/20294786/187699289-9a24ff22-aa49-48f4-ae91-38e8061d8467.png)

**ConvNets input size constraints**

**이렇게 해서 전체 이미지를 볼 수 있다면 training에는 왜 적용하지 않았었을까..??**

![스크린샷 2022-08-31 오후 10.37.52.png](https://user-images.githubusercontent.com/20294786/187699297-c14d3fa8-a4e8-40d5-8b0d-305d60002f2c.png)

![**Averaged pooling**](https://user-images.githubusercontent.com/20294786/187699300-cdffb4f0-f290-4c73-90df-fde717d5a1ec.png)

**Averaged pooling**

FC layer를 conv layer로 변경함으로써 입력 이미지를 고정시키지 않아도 되는데 output 사이즈가 상이할 수 있다. 이때 averaged pooling 연산을 통해서 1 x 1 x class_num 의 형태를 만들어 준다.

---

## 6. Experiments

![스크린샷 2022-08-31 오후 10.40.20.png](https://user-images.githubusercontent.com/20294786/187699302-466b95ba-52c2-4a14-957c-a35898110af3.png)

AlexNet에서 사용한 LRN, 1 x 1 Conv, layer의 수에 따라서 6가지 모델에 대해서 비교를 한 것이다.

![스크린샷 2022-08-31 오후 10.41.13.png](https://user-images.githubusercontent.com/20294786/187699308-6b878a1b-67f3-4502-8f13-f2c57b048979.png)

- Single Scale Training 방식일때는 S = Q
- Multi Scale Training 방식일때는 384 즉, 0.5 * (Smin + Smax)

첫 번째로는 Single test scale이다.

test시 image size가 256 또는 384의 크기로 고정되어 있는 것을 의미한다.

Single Scale Training 방식일때는 S = Q로 Multi Scale Training 방식일때는 384 즉, 0.5 * (Smin + Smax) 로 설정하였다.

AlexNet에서 사용되는 Local Response Normalization 기법을 사용한 **A-LRN 모델**과 그렇지 않은 **A 모델** 간의 성능차가 나지 않았기 때문에 **B 모델**부터는 사용하지 않았다.

또한, 1 x 1 conv filter를 사용하는 **C 모델**보다 3 x 3 Conv filter를 사용한 **D 모델**의 성능이 더 좋게 나온 것을 볼 수 있는데

![스크린샷 2022-08-31 오후 10.42.36.png](https://user-images.githubusercontent.com/20294786/187699312-9475398a-37bd-4104-a93e-7fdb783a9669.png)

그 이유를 1 x 1 Conv Filter를 사용하면 non linearity를 더 잘 표현할 수는 있지만, 3 x 3 conv filter가 spatial context를 더 잘 추출할 수 있기 때문이라고 언급하고 있다.

또한, 19 layer보다 더 깊이 layer를 쌓는 것은 무의미하다고 언급하고 있다. 

![스크린샷 2022-08-31 오후 10.43.18.png](https://user-images.githubusercontent.com/20294786/187699316-fc85c2e7-2d4b-4445-995d-28f9595b9fa5.png)

하나의 S 사이즈에 대해서 여러 scale로 test를 한 것인데,

![스크린샷 2022-08-31 오후 10.44.10.png](https://user-images.githubusercontent.com/20294786/187699321-045635b7-c0d1-4be0-a44a-0eed58952a55.png)

저자가 언급했듯이 하나의 S 사이즈에 대해서 여러 scale로 test를 한 경우 더 좋은 성능을 보인 것을 확인 할 수 있다.

![스크린샷 2022-08-31 오후 10.44.34.png](https://user-images.githubusercontent.com/20294786/187699325-80e6d109-04f5-4acd-bb18-ab9131b59922.png)

최종적으로 multi-crop과 dense를 결합한 “ConvNet fusion”이 가장 좋은 성능을 보여주고 있다.

![스크린샷 2022-08-31 오후 10.45.02.png](https://user-images.githubusercontent.com/20294786/187699326-1157dd97-0dfe-45a6-8d21-d8be66c16a83.png)

저자는 ILSVRC 대회에 모델성능을 제출할 때에 7개의 모델에 대해서 ensemble 기법을 적용했었는데, 대회가 끝나고 자체 실험에서 단 2개의 모델(**D & E**)만 **ensemble**한 결과가 더 좋았다고 언급하고 있다.

![스크린샷 2022-08-31 오후 10.45.25.png](https://user-images.githubusercontent.com/20294786/187699329-bc0d55aa-ce3e-4080-bb0c-12313399f9f9.png)

결과적으로 2014 ILSVRC에 출시된 classification 모델들과 경합한 결과 **VGGNet**가 **2등**을 차지하였다.
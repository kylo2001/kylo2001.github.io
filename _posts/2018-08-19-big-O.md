---
layout: post
title:  "Big - O"
date:   2018-08-19 23:16:00
categories: algorithm
tags: featured
image: /assets/article_images/2014-08-29-welcome-to-jekyll/desktop.JPG
comments: true

---


## Big-O

big-0 시간은 알고리즘의 효율성을 나타내는 지표 혹은 언어다. 이를 제대로 이 해하지 못하면 알고리즘을 개발하는 데 큰 고비를 겪을 수 있다. big-O를 몰라 가혹한 비판을 당할지도 모를 뿐만 아니라, 여러분의 알고리즘이 이전보다 빨라졌는 지 느려졌는지 판단하는 데도 어려움을 겪을 수 있다. 이 개념을 완벽히 익히길 바란다.



▶ **비유하기** 

디스크에 있는 파일을 다른 지역에 살고 있는 친구에게 가능하면 빨리 보내려고 한다고 가정해 보자. 어떻게 보낼 것인가?

대부분의 사람들은 문제를 듣자마자 이메일, FTP, 다른 온라인을 통한 전송 방 식을 생각하곤 한다. 그럴 듯하지만 이 방법은 경우에 따라 맞을 수도 틀릴 수도 있다.

만약 파일 크기가 작다면 당연히 온라인을 통한 전송이 빠를 것이다. 친구에게 직접 전달하려면 공항으로 가서 비행기에 오른 뒤 친구가 있는 곳까지 날아가야 하는데, 미국 내를 기준으로 했을 때 5~10시간 정도로 긴 시간이 걸리기 때문이다.

하지만 파일 크기가 아주 크다면 어떨까? 비행기를 통해 물리적으로 배달하는 게 더 빠를 수도 있지 않을까??

맞다. 실제로 그렇다. 1테라바이트 크기의 파일을 온라인을 통해 전송하려고 한다면 하루 이상 걸릴 수 있다. 비행기를 타고 날아가는 것이 더 빠를지도 모른다. 만약 정말 급한 상황이라면(그리고 비용이 큰 문제가 되지 않는다면), 비행기를 타 는 게 나을지도 모른다.

만약 비행편이 존재하지 않고 운전을 해야 하는 상황이라면 어떨까? 파일 사이즈 가 무지막지하게 크다면, 운전하는 편이 더 빠를 수도 있다.



▶ **시간 복잡도**

이것이 바로 점근적 실행 시간(asymptotic runtime), 또는 big-0 시간에 대한 개념이다. 우리는 데이터 전송 알고리즘'의 실행 시간을 다음과 같이 설명할 수 있다.

**·** **온라인 전송**: O(s), 여기서 s는 파일의 크기가 된다. 즉, 파일의 크기가 증가함에 따라 전송 시간 또한 선형적으로 증가한다.

**·** **비행기를 통한 전송**: 파일 크기에 관계없이 O(1), 파일의 크기가 증가한다고 해

서 친구에게 파일을 전송하는 데 걸리는 시간이 늘어나지 않는다. 즉, 상수 시간 만큼 소요된다.

상수가 얼마나 큰지 아니면 선형식이 얼마나 천천히 증가하는지에 관계없이 숫자가 커지다 보면 선형식은 언젠가 상수를 뛰어넘게 된다. 그 외에 다양한 종류의 실행 시간이 존재한다. 가장 흔하게 사용되는 것 몇 가지를

![](https://user-images.githubusercontent.com/20294786/44307061-ee3b6980-a3d6-11e8-94db-701018f75a7d.png)

예로 들자면 O(logN), O(NlogN), O(N), O(N^2), O(2)이 있다. 이 외에도 다양한 실행 시간이 존재할 수 있다. 가능한 실행 시간이 딱 정해져 있는 것이 아니다.

실행 시간에 다양한 변수가 포함될 수도 있다. 예를 들어, 너비가 w미터이고 높이가 h미터인 울타리를 색칠하는 데 소요되는 시간은 O(wh)로 표기할 수 있다. 만 약 페인트를 p번 덧칠해야 한다면, O(whp)의 시간이 소요된다고 말할 수 있다.

big-o, big-⊝, big-Ω 학업 과정 중에 big-O를 공부했던 적이 없다면 이 부분은 넘어가도 좋다. 처음 big-O를 접하는 사람에게는 도움이 되기보단 혼란만 가중시킬지도 모른다. 여기에 나와 있는 대부분의 내용은 이전에 big-0 개념을 공부했던 사람들을 위한 ‘참고 사 항'이다. 이를 통해 단어 선택의 모호함을 없애고, “내 생각에 big-O의 개념은 ...”과 같은 말을 하게 되는 것을 사전에 방지하고자 한다.

학계에서는 수행 시간을 표기할 때 big-0, big-⊝(theta), big-Ω(omega)를 사용 한다.

• O(big-O): 학계에서 big-O는 시간의 상한을 나타낸다. 배열의 모든 값을 출력 하는 알고리즘은 O(N)으로 표현할 수 있지만, 이 외에 N보다 큰 big-O 시간으 로 표현할 수도 있다. 예를 들어, O(N^2), O(N^3), O(2^N)도 옳은 표현이다. 다시 말해 알고리즘의 수행 시간은 적어도 이들 중 하나보다 빠르기만 하면 된다. 따라서 big-O 시간은 알고리즘 수행 시간의 상한이 되고, 이는 ‘작거나 같은’ 부등호 와도 비슷한 관계가 있다. 만약 밥(Bob)의 나이가 X라면(일반적으로 사람의 나이는 기껏해야 130살이라고 가정하자), X <= 130이라고 말할 수 있지만, X <= 1,000 혹은 X <= 1,000,000으로 나타내도 옳은 표기법이다. 수학적으로 얘기해서 이들 모두 참(true)인 표현법이다(물론 굉장히 쓸모 없는 표기법이지만), 마찬가지로, 배열의 모든 값을 출력하는 알고리즘은 O(N)이라고 표현할 수 있을 뿐만 아니라 O(N) 혹은 O(N)보다 큰 어떤 수행 시간으로도 표현 가능하다. 

• Ω(big-Omega): 학계에서 Ω는 등가 개념 혹은 하한을 나타낸다. 배열의 모든 값을 출력하는 알고리즘은 Ω(N)뿐만 아니라 Ω(logN) 혹은 Ω(1)로도 표현할 수있다. 결국, 해당 알고리즘은 Ω 수행 시간보다 빠를 수 없게 된다. 

• ⊝(big theta): 학계에서 ⊝는 O와 Ω 둘 다 의미한다. 즉, 어떤 알고리즘의 수행 시간이 O(N)이면서 Ω(N)이라면, 이 알고리즘의 수행 시간을 ⊝(N)로 표현할 수 있다.

업계에서는(즉 면접에서는) ⊝와 O를 하나로 합쳐 표현하려는 것 같다. 업계에서 big-O의 의미는 학계에서 ⊝의 의미와 가깝고, 그래서 배열을 출력하는 알고리즘을 O(N)이라고 부르는 것은 잘못된 표현처럼 보인다. 업계에서는 그냥 O(N)으로 표현한다. 우리는 업계의 추세를 따라서 big-O 표기법을 사용할 것이다. 즉, 수행시간은 언제나 딱 맞게 표기하려 할 것이다.



**최선의 경우, 최악의 경우, 평균적인 경우**

실제로 우리는 알고리즘의 수행 시간을 세 가지 다른 방법으로 나타낼 수 있다.

퀵 정렬(quick sort)의 관점에서 살펴보자. 퀵 정렬은 '축'이 되는 원소 하나를 무작위로 뽑은 뒤 이보다 작은 원소들은 앞에, 큰 원소들은 뒤에 놓이도록 원소의 위치를 바꾼다. 그 결과 '부분 정렬(partial sort)’이 완성되고, 그 뒤 왼쪽과 오른쪽 부분을 이와 비슷한 방식으로 재귀적으로 정렬해 나간다.

• 최선의 경우: 만약 모든 원소가 동일하다면 퀵 정렬은 평균적으로 단순히 배열을 한 차례 순회하고 끝날 것이다. 즉, 수행 시간이 O(N)이 된다(실제론 퀵 정렬의 구현 방식에 따라 조금 달라질 수 있다. 배열이 정렬되어 있을 때 굉장히 빠르게 동작하는 구현 방식도 존재한다).

• 최악의 경우: 만약 운이 없게도 배열에서 가장 큰 원소가 계속해서 축이 된다면 어떠할까? (실제로, 쉽게 일어날 수 있는 일이다. 배열이 역순으로 정렬되어 있 고 부분 배열의 첫 번째 원소를 항상 축으로 잡는다면 이런 일이 발생할 수 있 다.) 이런 경우에는 재귀 호출이 배열을 절반 크기의 부분 배열로 나누지 못 하고, 고작 하나 줄어든 크기의 부분 배열로 나누게 된다. 따라서 수행 시간은 O(N^2)으로 악화된다.

• 평균적인 경우: 하지만 보통 위와 같이 아주 멋지거나 끔찍한 경우는 자주 발생하지 않는다. 물론, 가끔 축이 되는 원소가 가장 작거나 가장 클 순 있지만, 이런 일이 반복적으로 일어나는 경우가 많지 않다. 따라서 수행 시간이 평균적으로 O(NlogN)이라고 말할 수 있다.

최선의 경우에 시간 복잡도는 별로 쓸만한 개념이 아닌 탓에 좀처럼 논의 대상이 되지 않는다. 최선의 경우는 결국 아무 알고리즘이나 취한 뒤 특수한 입력을 넣으면, O(1) 시간에 동작하도록 만들 수 있다.

많은(아마도 대부분) 알고리즘은 최악의 경우와 평균적인 경우가 같다. 가끔 이들이 달라서, 최악과 평균 두 경우 모두 언급해야 되기도 하지만 말이다.

최선의/최악의/평균적인 경우와 big-0/⊝/0 사이의 관련성 

지원자의 입장에서 이들 개념에 혼돈이 오기 쉽지만(아마도 이들 모두 '더 높은 (higher)', '더 낮은’(lower), 꼭 맞는'(exactly right)과 같은 개념이 있기 때문일 것 이다), 사실 이들 사이에 특별한 관련성은 없다.

최선의, 최악의, 평균의 경우는 어떤 입력 혹은 상황에 대해서 big-O(혹은 big theta) 시간으로 설명한다.

big-O, big-⊝, big-Ω는 각각 상한, 하한, 딱 맞는 수행 시간을 의미한다.



▶ **공간 복잡도**

알고리즘에서는 시간뿐 아니라 메모리(혹은 공간) 또한 신경 써야 한다. 공간 복잡도는 시간 복잡도와 평행선을 달리는 개념이다. 크기가 n인 배열을 만들고자 한다면, O(n)의 공간이 필요하다. n x n 크기의 2차원 배열을 만들고자 한다 면, O(n^2)의 공간이 필요하다.

재귀 호출에서 사용하는 스택 공간 또한 공간 복잡도 계산에 포함된다. 예를 들어, 다음과 같은 코드는 O(n) 시간과 O(n) 공간을 사용한다.

```
/* 예제 1 */
int sum(int n) { 
    if (n <= 0)  { 
        return 0;
    }
	return n + sum(n-1);
}
```

호출될 때마다 스택의 깊이는 깊어진다.

```
sum(4)
        → sum(3)
                →sum(2)
                        →sum(1)
                                →sum(0)
```

위의 호출은 전부 호출 스택에 더해지고 실제 메모리 공간을 잡아 먹는다. 하지만 단지 n번 호출했다고 해서 O(n) 공간을 사용한다고 말할 순 없다. 0과 n 사이에서 인접한 두 원소의 합을 구하는 아래 함수를 살펴보자.

```
/* 예제 2 */
int pairSumSequence(int n) { 
	int sum = 0; 
 	for (int i = 0; i < n; i++) {
		sum += pairSum(i, i + 1);
	}
	return sum;
}

int pairSum(int a, int b) {
	return a + b;
}
```

위의 코드는 pairSum 함수를 대략 O(n)번 호출했지만, 이 함수들이 호출 스택에 동시에 존재하지는 않으므로 O(1) 공간만 사용한다.



▶ **상수항은 무시하라** 

big-O는 단순히 증가하는 비율을 나타내는 개념이므로, 특수한 입력에 한해 O(N) 코드가 O(1) 코드보다 빠르게 동작하는 것은 매우 가능성 있는 얘기다. 이런 이유로 우리는 수행 시간에서 상수항을 무시해 버린다. 즉, O(2N)으로 표기 되어야 할 알고리즘을 실제로는 O(N)으로 표기한다.

많은 사람들이 이런 표기를 달가워하지 않는다. 두 개의 (중첩되지 않은) 루프로 이루어진 코드가 있을 때 어떤 이들은 여전히 O(2N)으로 표기한다. 그들은 O(2N) 이 더 정확한 표기법이라고 생각한다. 하지만 그렇지 않다. 

아래의 코드를 살펴보자.



```
/* 최소와 최대 1 */

int min = Integer.MAX_VALUE; 
int max = Integer.MIN_VALUE; 

for (int x : array) {
	if (x < min) min = x; 
	if (x > max) max = X;
}
```

```
/* 최소와 최대 2 */

int min = Integer.MAX_VALUE;
int max = Integer.MIN_VALUE;

for (int x : array) {
	if (x < min) min = x;
}
for (int x : array) { 
	if (x > max) max = X;
}
```

무엇이 더 빠른가? 첫 번째 코드는 for 루프를 한 개 사용했고 두 번째 코드는 for 루프를 두 개 사용했다. 첫 번째 방법은 루프 안에 코드가 두 줄이 들어 있는 반면, 두 번째 방법은 루프 안에 코드가 한 줄이 들어 있다. 실행되는 명령어(instruction)의 개수를 직접 세어 보려면, 어셈블리(assembly) 단계에서 곱셈이 덧셈보다 더 많은 명령어를 사용한다는 점, 컴파일러가 나름의 최적화를 한다는 점, 그리고 모든 다른 종류의 세부사항들 또한 고려해야 한다. **이런 작업은 몹시 복잡하므로 분석할 생각조차 하지 말길 바란다.** big-O 표기법은 수행 시간이 어떻게 변화하는지를 표현해주는 도구이다. 

**따라서 O(N)이 언제나 O(2N)보다 나은 것은 아니라는 사실만 받아들이면 된다.**



▶ **지배적이지 않은 항은 무시하라**

 O(N^2+N)과 같은 수식이 있을 때는 어떻게 해야 할까? 두 번째 N은 틀림없이 상수항은 아니지만 그렇다고 특별히 중요한 항도 아니다. 앞에서 상수항은 무시해도 된다고 언급했다. 따라서 O(N^2+N^2)은 O(N^2)이 된다. 이처럼 마지막 N^2항을 무시해도 된다면, 그보다 작은 N항은 어떨까? 무시해도 된다. 수식에서 지배적이지 않은 항은 무시해도 된다.



• O(N^2+N^2)은 O(N^2)이 된다.

• O(N+logN)은 O(N)이 된다. 

• O(5*2^N + 1000N^100)은 0(2^N)이 된다.



여전히 수식에 합이 남아 있을 수 있다. 예를 들어, O(B^2 + A)는 하나의 항으로 줄일 수 없다(A와 B 사이에 존재하는 특별한 관계를 알고 있지 않는 이상).

다음의 그래프는 흔히 사용되는 big-O 시간의 증가율을 나타낸다.



![](https://user-images.githubusercontent.com/20294786/44308282-6e6cc980-a3ed-11e8-9c51-079d77dc9bab.png)

그래프에서 보듯이 O(x^2)은 O(x)보다 많이 느리지만, O(2^x)이나 O(x!)보다는 느리지 않다. O(x^x)이나 O(2^x*x!)처럼 O(x!)보다 수행 시간이 느린 경우는 생각보다 많이 존재한다.



▶ **여러 부분으로 이루어진 알고리즘: 덧셈 vs 곱셈**

어떤 알고리즘이 두 단계로 이루어져 있다고 가정하자. 어떤 경우에 수행 시간을 곱하고 어떤 경우에 더해야 하나? 

이는 많은 사람들이 혼란스러워 하는 부분이다.

```
/* 덧셈 수행 시간: O(A + B) */

for (int a : arrA) {
	print(a);
}

for (int b : arrB) {
	print(b);
}
```

```
/* 곱셈 수행 시간: O(A * B) */

for (int a : arrA) {
	for (int b : arrB) {
		print(a + "," + b);
	}
}
```

왼쪽의 예제에선 A의 일을 한 뒤에 B의 일을 수행한다. 따라서 전체 수행한 일은 O(A+B)가 된다. 오른쪽 예제에선 A의 각 원소에 대해 B의 일을 수행한다. 따라서 전체 수행한 일은 O(A*B)가 된다. 다시 말하면 다음과 같다.



• 만약 알고리즘이 “A 일을 모두 끝마친 후에 B 일을 수행하라” 의 형태라면 A와 B

의 수행 시간을 더해야 한다. 

• 만약 알고리즘이 “A 일을 할 때마다 B 일을 수행하라” 의 형태라면 A와 B의 수행

시간을 곱해야 한다.

실제 면접에서 실수하기 쉬운 내용이므로 꼭 조심하길 바란다.



▶ **상환 시간** 

ArrayList(동적 가변크기 배열)는 배열의 역할을 함과 동시에 크기가 자유롭게 조절되는 자료구조이다. 원소 삽입 시 필요에 따라 배열의 크기를 증가시킬 수 있기 때문에 ArrayList의 공간이 바닥날 일은 없다. 

ArrayList는 배열로 구현되어 있다. 배열의 용량이 꽉 찼을 때, ArrayList 클래스는 기존보다 크기가 두 배 더 큰 배열을 만든 뒤, 이전 배열의 모든 원소를 새 배열 로 복사한다. 

이때 삽입 연산의 수행 시간은 어떻게 되겠는가? 약간 까다로운 문제다. 

배열이 가득 차 있는 경우를 생각해 보자. 배열에 N개의 원소가 들어 있을 때 새로운 원소를 삽입하려면 O(N)이 걸린다. 왜냐하면 크기가 2N인 배열을 새로 만들고 기존의 모든 원소를 새 배열로 복사해야 하기 때문이다. 따라서 이 경우에 삽입 연산은 O(N) 시간이 소요된다. 

하지만 위에서처럼 배열이 가득 차 있는 경우는 극히 드물다. 대다수의 경우에는 배열에 가용 공간이 존재하고 이때의 삽입 연산은 O(1)이 걸린다. 이제 두 가지 경우를 포함한 전체 수행 시간을 따져봐야 하는데, 여기서 상환 시 간이라는 개념을 이용하면 쉽게 구할 수 있다. 

최악의 경우는 가끔 발생하지만 한번 발생하면 그 후로 꽤 오랫동안 나타나지 않으므로 비용(수행 시간)을 분할 상환 한다는 개념이다. 

위의 경우에 상환 시간은 무엇이 되겠는가? 

배열의 크기가 2의 승수가 되었을 때 원소를 삽입하면 용량이 두 배로 증가한다. 

즉, 배열의 크기가 1, 2, 4, 8, 16, ..., X일 때 새로운 원소를 삽입하면 배열의 용량이 두 배로 증가하고, 이때 기존의 1, 2, 4, 8, 16, 32, 64, ..., X개의 원소 또한 새로운 배열로 복사해줘야 한다. 합을 구해 보자. 1 + 2 + 4 + 8 + 16 + ... + X의 합은 무엇인가? 

이 수열을 왼쪽에서 오른쪽으로 차례로 읽으면 1에서 시작해서 X가 될 때까지 두 배씩 증가하는 수열이 된다. 반대로 오른쪽에서 왼쪽으로 읽으면 X에서 시작해서 1이 될 때까지 절반씩 줄어드는 수열이 된다. X + X/2 + X/4 + X/8 + … + 1의 합은 대략 2X와 같다. 따라서 X개의 원소를 삽입했을 때 필요한 시간은 0(2X)이고, 이를 분할 상환해보면 삽입 한 번에 필요한 시간은 O(1)이다.



▶ log N 수행 시간 

우리는 O(logN) 수행 시간을 자주 접한다. 이 수행 시간은 어떻게 구해진 걸까? 이진 탐색(binary search)을 생각해 보자. 이진 탐색은 N개의 정렬된 원소가 들어 있는 배열에서 원소 X를 찾을 때 사용된다. 먼저 원소 x와 배열의 중간값을 비교한다. ‘x == 중간값’을 만족하면 이를 반환한다. ‘x < 중간값'일 때는 배열의 왼쪽 부분을 재탐색하고 ‘x > 중간값’일 경우에는 배열의 오른쪽 부분을 재탐색한다.

```
{1, 5, 8, 9, 11, 13, 15, 19, 21}에서 9 검색하기
	9와 11을 비교해본다  →  9가 11보다 작다. 
	11보다 작거나 같은 {1, 5, 8, 9, 11}에서 9 검색하기
		이번에는 9와 8을 비교해본다  →  9가 8보다 크다.
		8보다 크고 11보다 작거나 같은 {9, 11}에서 9 검색하기
			9와 9를 비교해본다. -> 9를 찾았다!!!! 
			결과를 반환한다.
```

처음에는 원소 N개가 들어 있는 배열에서 시작한다. 한 단계가 지나면 탐색해야 할 원소의 개수가 N/2로 줄어들고, 한 단계가 더 지나면 N / 4개로 줄어든다. 그러다가 원소를 찾았거나 탐색해야 할 원소가 하나만 남으면 탐색을 중지한다.

총 수행 시간은 N을 절반씩 나누는 과정에서 몇 단계 만에 1이 되는지에 따라 결정된다.

```
N = 16 
N = 8 /* 나누기 2 */ 
N = 4 /* 나누기 2 */ 
N = 2 /* 나누기 2 */ 
N = 1 /* 나누기 2 */
```

16에서 1로 감소하는 순서를 뒤집어서 1에서 16으로 증가하는 순서로 생각해 보자. 숫자 1에 2를 몇 번 곱해야 N이 될까?

```
N= 1 
N = 2 /* 곱하기 2 */
N = 4 /* 곱하기 2 */ 
N = 8 /* 곱하기 2 */ 
N = 16 /* 곱하기 2 */
```



즉, 2^k = N을 만족하는 k는 무엇인가? 이때 사용되는 것이 바로 로그(log)이다.

2^4 = 16 → log2(16) = 4  (log2()은 로그의 밑이 2인 로그를 뜻한다.)

log2(N) = k → 2^k = N

필요할 때마다 가져다 쓰기 좋은 개념이다. 어떤 문제에서 원소의 개수가 절반씩 줄어든다면 그 문제의 수행 시간은 O(logN)이 될 가능성이 크다. 같은 원리로, 균형 이진 탐색 트리(balanced binary search tree)에서 원소를 찾는 문제도 O(logN)이다. 매 단계마다 원소의 대소를 비교한 뒤 왼쪽 혹은 오른쪽으로 내려간다. 각 단계에서 검색해야 할 노드의 개수가 절반씩 줄어들게 되므로, 문제 공간(problem space) 또한 절반씩 줄어든다.

여기서 말하는 로그의 밑은 무엇일까? 아주 좋은 질문이다! 간단히 대답하면 big-O에선 로그의 밑을 고려할 필요가 없다고 보면 된다. 



▶ 재귀적으로 수행 시간 구하기 

여기 수행 시간을 구하기 약간 까다로운 코드가 있다.

```
int f(int n) {
	if (n <= 1) {
		 return 1;
	}
	return f(n - 1) + f(n - 1);
}
```

함수 f가 두 번 호출된 것을 보고 많은 사람들이 성급하게 O(N^2)이라고 결론 내릴 것이다. 완전 틀렸다.

수행 시간을 추측하지 말고 코드를 하나씩 읽어 나가면서 수행 시간을 계산해보자. f(4)는 f(3)을 두 번 호출한다. f(3)은 f(2)를 거쳐서 결국 f(1)까지 호출한다. 이 트리에 나타난 총 호출 횟수는 몇 개인가? 일일이 세지 말고 계산해 보자!

![](https://user-images.githubusercontent.com/20294786/44308531-767b3800-a3f2-11e8-89be-10221b74f8fc.png)



트리의 깊이가 N이고, 각 노드(즉, 함수 호출)는 두 개의 자식 노드를 갖고 있다. 따 라서 깊이가 한 단계 깊어질 때마다 이전보다 두 배 더 많이 호출하게 된다. 같은 높이에 있는 노드의 개수를 세어 보면 아래와 같다.

![](https://user-images.githubusercontent.com/20294786/44308736-f0f98700-a3f5-11e8-885b-a5e7400a5305.png)



따라서 전체 노드의 개수는 2^0 + 2^1 + 2^2 + 2^3 + 2^4 + ... +2^N (= 2^N+1 - 1)이 된다. 

이 패턴을 잊지 말길 바란다. 다수의 호출로 이루어진 재귀 함수에서 수행 시간

은 보통(물론 항상 그런 건 아니지만)  O(분기^깊이)로 표현되곤 한다. 여기서 분기 (branch)란 재귀 함수가 자신을 재호출하는 횟수를 뜻한다. 따라서 위의 경우에 수행 시간은 O(2^N)이 된다.

기억할지도 모르겠지만, 로그의 밑은 상수항으로 취급되기 때문에 big-O 표 기법에서 로그의 밑은 무시해도 된다고 했었다. 하지만 지수(exponent)에서는 얘기가 달라진다. 지수의 밑은 무시하면 안 된다. 이해를 돕기 위해 2^N과 8^N을 비교해 보자. 8^N은 (2^3)^N, 2^3N, 2^2N*2^N 으로도 표현할 수 있다. 보는 바와 같 이 8^N은 2^N에 2^2N을 곱한 것과 같으므로 2^N과 8^N 사이에는 2^2N만큼의 차이가 있다. 이 차이(2^2N)는 지수항이므로 상수항과는 아주 큰 차이가 있다.

이 알고리즘의 공간 복잡도는 O(N)이 될 것이다. 전체 노드의 개수는 O(2^N)이지만, 특정 시각에 사용하는 공간의 크기는 O(N)이다. 따라서 필요한 가용 메모리의 크 기는 O(N)이면 충분할 것이다.



참고 도서 : Cracking The Coding Interview 6/E
